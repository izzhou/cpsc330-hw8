{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw8.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning\n",
    "\n",
    "## Homework 8: Introduction to Computer vision and Time Series (Lectures 19 and 20) \n",
    "\n",
    "**Due date: see the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330/blob/master/docs/calendar.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2023W1/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work in a group on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission.\n",
    "4. Make sure that the plots and output are rendered properly in your submitted file. \n",
    "5. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Exercise 1: time series prediction\n",
    "\n",
    "In this exercise we'll be looking at a [dataset of avocado prices](https://www.kaggle.com/neuromusic/avocado-prices). You should start by downloading the dataset and storing it under the `data` folder. We will be forcasting average avocado price for the next week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>1.33</td>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-20</td>\n",
       "      <td>1.35</td>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-06</td>\n",
       "      <td>1.08</td>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-29</td>\n",
       "      <td>1.28</td>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AveragePrice  Total Volume     4046       4225    4770  \\\n",
       "0 2015-12-27          1.33      64236.62  1036.74   54454.85   48.16   \n",
       "1 2015-12-20          1.35      54876.98   674.28   44638.81   58.33   \n",
       "2 2015-12-13          0.93     118220.22   794.70  109149.67  130.50   \n",
       "3 2015-12-06          1.08      78992.15  1132.00   71976.41   72.58   \n",
       "4 2015-11-29          1.28      51039.60   941.48   43838.39   75.78   \n",
       "\n",
       "   Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  region  \n",
       "0     8696.87     8603.62       93.25          0.0  conventional  2015  Albany  \n",
       "1     9505.56     9408.07       97.49          0.0  conventional  2015  Albany  \n",
       "2     8145.35     8042.21      103.14          0.0  conventional  2015  Albany  \n",
       "3     5811.16     5677.40      133.76          0.0  conventional  2015  Albany  \n",
       "4     6183.95     5986.26      197.69          0.0  conventional  2015  Albany  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/avocado.csv\", parse_dates=[\"Date\"], index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18249, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-01-04 00:00:00')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-03-25 00:00:00')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data ranges from the start of 2015 to March 2018 (~2 years ago), for a total of 3.25 years or so. Let's split the data so that we have a 6 months of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '20170925'\n",
    "df_train = df[df[\"Date\"] <= split_date]\n",
    "df_test  = df[df[\"Date\"] >  split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_train) + len(df_test) == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 How many time series? \n",
    "rubric={points:4}\n",
    "\n",
    "In the [Rain in Australia](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package) dataset from lecture demo, we had different measurements for each Location. \n",
    "\n",
    "We want you to consider this for the avocado prices dataset. For which categorical feature(s), if any, do we have separate measurements? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have different measurements for each type of avocado. For example, for each region and Date combination, we have 2 examples of data, one for conventional type of avocado, one for organic type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1373.95</td>\n",
       "      <td>57.42</td>\n",
       "      <td>153.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1162.65</td>\n",
       "      <td>1162.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1182.56</td>\n",
       "      <td>39.00</td>\n",
       "      <td>305.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>838.44</td>\n",
       "      <td>838.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "51 2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "51 2015-01-04          1.79       1373.95    57.42    153.88    0.00   \n",
       "50 2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "50 2015-01-11          1.77       1182.56    39.00    305.12    0.00   \n",
       "49 2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "\n",
       "    Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "51     9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "51     1162.65     1162.65        0.00          0.0       organic  2015   \n",
       "50     8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "50      838.44      838.44        0.00          0.0       organic  2015   \n",
       "49    11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "\n",
       "    region  \n",
       "51  Albany  \n",
       "51  Albany  \n",
       "50  Albany  \n",
       "50  Albany  \n",
       "49  Albany  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by = [\"region\", \"Date\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 Equally spaced measurements? \n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain in Australia dataset, the measurements were generally equally spaced but with some exceptions. How about with this dataset? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is generally equally spaced with only one exception: WestTexNewMexico, organic type of avocado, there's one existence has 14 days in between, and another one has 21 days in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_time_spacing_for_all(df):\n",
    "    \"\"\"\n",
    "    Prints the distribution of time spacing for each region and type.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame with columns 'region', 'type', and 'Date'.\n",
    "    \"\"\"\n",
    "    # Ensure 'Date' is in datetime format\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Get unique combinations of region and type\n",
    "    regions = df['region'].unique()\n",
    "    types = df['type'].unique()\n",
    "    \n",
    "    for region in regions:\n",
    "        for type_ in types:\n",
    "            # Filter data for the given region and type\n",
    "            region_type_data = df[(df['region'] == region) & (df['type'] == type_)]\n",
    "            \n",
    "            if region_type_data.empty:\n",
    "                continue  # Skip if there's no data for this combination\n",
    "            \n",
    "            # Calculate time differences\n",
    "            time_diffs = region_type_data['Date'].sort_values().diff().dropna()\n",
    "            \n",
    "            # Count the frequency of each time difference\n",
    "            value_counts = time_diffs.value_counts().sort_index()\n",
    "            \n",
    "            # Print the results\n",
    "            print(f\"Time spacing counts for region: {region}, type: {type_}:\\n{value_counts}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spacing counts for region: Albany, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Albany, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Atlanta, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Atlanta, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: BaltimoreWashington, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: BaltimoreWashington, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Boise, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Boise, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Boston, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Boston, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: BuffaloRochester, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: BuffaloRochester, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: California, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: California, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Charlotte, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Charlotte, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Chicago, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Chicago, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: CincinnatiDayton, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: CincinnatiDayton, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Columbus, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Columbus, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: DallasFtWorth, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: DallasFtWorth, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Denver, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Denver, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Detroit, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Detroit, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: GrandRapids, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: GrandRapids, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: GreatLakes, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: GreatLakes, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: HarrisburgScranton, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: HarrisburgScranton, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: HartfordSpringfield, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: HartfordSpringfield, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Houston, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Houston, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Indianapolis, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Indianapolis, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Jacksonville, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Jacksonville, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: LasVegas, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: LasVegas, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: LosAngeles, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: LosAngeles, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Louisville, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Louisville, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: MiamiFtLauderdale, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: MiamiFtLauderdale, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Midsouth, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Midsouth, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Nashville, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Nashville, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: NewOrleansMobile, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: NewOrleansMobile, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: NewYork, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: NewYork, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Northeast, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Northeast, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: NorthernNewEngland, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: NorthernNewEngland, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Orlando, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Orlando, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Philadelphia, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Philadelphia, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: PhoenixTucson, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: PhoenixTucson, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Pittsburgh, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Pittsburgh, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Plains, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Plains, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Portland, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Portland, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: RaleighGreensboro, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: RaleighGreensboro, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: RichmondNorfolk, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: RichmondNorfolk, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Roanoke, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Roanoke, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Sacramento, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Sacramento, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: SanDiego, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: SanDiego, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: SanFrancisco, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: SanFrancisco, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Seattle, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Seattle, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: SouthCarolina, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: SouthCarolina, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: SouthCentral, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: SouthCentral, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Southeast, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Southeast, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Spokane, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Spokane, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: StLouis, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: StLouis, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Syracuse, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Syracuse, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Tampa, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: Tampa, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: TotalUS, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: TotalUS, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: West, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: West, type: organic:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: WestTexNewMexico, type: conventional:\n",
      "Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time spacing counts for region: WestTexNewMexico, type: organic:\n",
      "Date\n",
      "7 days     163\n",
      "14 days      1\n",
      "21 days      1\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_time_spacing_for_all(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Interpreting regions \n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain in Australia dataset, each location was a different place in Australia. For this dataset, look at the names of the regions. Do you think the regions are also all distinct, or are there overlapping regions? Justify your answer by referencing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the names of the regions, there are overlapping regions. For example, Great Lakes is an area that covers cities like Detroit, Chicago etc. There are also both names for cities and states like SanDiego, SanFrancisco, and California. Also, the regions include Total US, West, Northeast, SouthCentral, Southeast etc., which are overlapping with states and cities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique regions: 54\n",
      "Frequency of each region:\n",
      "region\n",
      "Albany                 338\n",
      "Sacramento             338\n",
      "Northeast              338\n",
      "NorthernNewEngland     338\n",
      "Orlando                338\n",
      "Philadelphia           338\n",
      "PhoenixTucson          338\n",
      "Pittsburgh             338\n",
      "Plains                 338\n",
      "Portland               338\n",
      "RaleighGreensboro      338\n",
      "RichmondNorfolk        338\n",
      "Roanoke                338\n",
      "SanDiego               338\n",
      "Atlanta                338\n",
      "SanFrancisco           338\n",
      "Seattle                338\n",
      "SouthCarolina          338\n",
      "SouthCentral           338\n",
      "Southeast              338\n",
      "Spokane                338\n",
      "StLouis                338\n",
      "Syracuse               338\n",
      "Tampa                  338\n",
      "TotalUS                338\n",
      "West                   338\n",
      "NewYork                338\n",
      "NewOrleansMobile       338\n",
      "Nashville              338\n",
      "Midsouth               338\n",
      "BaltimoreWashington    338\n",
      "Boise                  338\n",
      "Boston                 338\n",
      "BuffaloRochester       338\n",
      "California             338\n",
      "Charlotte              338\n",
      "Chicago                338\n",
      "CincinnatiDayton       338\n",
      "Columbus               338\n",
      "DallasFtWorth          338\n",
      "Denver                 338\n",
      "Detroit                338\n",
      "GrandRapids            338\n",
      "GreatLakes             338\n",
      "HarrisburgScranton     338\n",
      "HartfordSpringfield    338\n",
      "Houston                338\n",
      "Indianapolis           338\n",
      "Jacksonville           338\n",
      "LasVegas               338\n",
      "LosAngeles             338\n",
      "Louisville             338\n",
      "MiamiFtLauderdale      338\n",
      "WestTexNewMexico       335\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "unique_regions = df['region'].unique()\n",
    "region_counts = df['region'].value_counts()\n",
    "\n",
    "print(f\"Total number of unique regions: {len(unique_regions)}\")\n",
    "print(f\"Frequency of each region:\\n{region_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the entire dataset despite any location-based weirdness uncovered in the previous part.\n",
    "\n",
    "We will be trying to forecast the avocado price. The function below is adapted from [Lecture 19](https://github.com/UBC-CS/cpsc330-2023W1/tree/main/lectures), with some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_lag_feature(df, orig_feature, lag, groupby, new_feature_name=None, clip=False):\n",
    "    \"\"\"\n",
    "    Creates a new feature that's a lagged version of an existing one.\n",
    "    \n",
    "    NOTE: assumes df is already sorted by the time columns and has unique indices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        The dataset.\n",
    "    orig_feature : str\n",
    "        The column name of the feature we're copying\n",
    "    lag : int\n",
    "        The lag; negative lag means values from the past, positive lag means values from the future\n",
    "    groupby : list\n",
    "        Column(s) to group by in case df contains multiple time series\n",
    "    new_feature_name : str\n",
    "        Override the default name of the newly created column\n",
    "    clip : bool\n",
    "        If True, remove rows with a NaN values for the new feature\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        A new dataframe with the additional column added.\n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    if new_feature_name is None:\n",
    "        if lag < 0:\n",
    "            new_feature_name = \"%s_lag%d\" % (orig_feature, -lag)\n",
    "        else:\n",
    "            new_feature_name = \"%s_ahead%d\" % (orig_feature, lag)\n",
    "    \n",
    "    new_df = df.assign(**{new_feature_name : np.nan})\n",
    "    for name, group in new_df.groupby(groupby):        \n",
    "        if lag < 0: # take values from the past\n",
    "            new_df.loc[group.index[-lag:],new_feature_name] = group.iloc[:lag][orig_feature].values\n",
    "        else:       # take values from the future\n",
    "            new_df.loc[group.index[:-lag], new_feature_name] = group.iloc[lag:][orig_feature].values\n",
    "            \n",
    "    if clip:\n",
    "        new_df = new_df.dropna(subset=[new_feature_name])\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first sort our dataframe properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18248</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.62</td>\n",
       "      <td>15303.40</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2171.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10806.44</td>\n",
       "      <td>10569.80</td>\n",
       "      <td>236.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18249 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "1     2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "2     2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "3     2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "4     2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "...          ...           ...           ...      ...       ...     ...   \n",
       "18244 2018-02-25          1.57      18421.24  1974.26   2482.65    0.00   \n",
       "18245 2018-03-04          1.54      17393.30  1832.24   1905.57    0.00   \n",
       "18246 2018-03-11          1.56      22128.42  2162.67   3194.25    8.93   \n",
       "18247 2018-03-18          1.56      15896.38  2055.35   1499.55    0.00   \n",
       "18248 2018-03-25          1.62      15303.40  2325.30   2171.66    0.00   \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0         9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "1         8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "2        11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "3        10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "4         9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "...           ...         ...         ...          ...           ...   ...   \n",
       "18244    13964.33    13698.27      266.06          0.0       organic  2018   \n",
       "18245    13655.49    13401.93      253.56          0.0       organic  2018   \n",
       "18246    16762.57    16510.32      252.25          0.0       organic  2018   \n",
       "18247    12341.48    12114.81      226.67          0.0       organic  2018   \n",
       "18248    10806.44    10569.80      236.64          0.0       organic  2018   \n",
       "\n",
       "                 region  \n",
       "0                Albany  \n",
       "1                Albany  \n",
       "2                Albany  \n",
       "3                Albany  \n",
       "4                Albany  \n",
       "...                 ...  \n",
       "18244  WestTexNewMexico  \n",
       "18245  WestTexNewMexico  \n",
       "18246  WestTexNewMexico  \n",
       "18247  WestTexNewMexico  \n",
       "18248  WestTexNewMexico  \n",
       "\n",
       "[18249 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sort = df.sort_values(by=[\"region\", \"type\", \"Date\"]).reset_index(drop=True)\n",
    "df_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then call `create_lag_feature`. This creates a new column in the dataset `AveragePriceNextWeek`, which is the following week's `AveragePrice`. We have set `clip=True` which means it will remove rows where the target would be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "      <th>AveragePriceNextWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18243</th>\n",
       "      <td>2018-02-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>17597.12</td>\n",
       "      <td>1892.05</td>\n",
       "      <td>1928.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13776.71</td>\n",
       "      <td>13553.53</td>\n",
       "      <td>223.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18141 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "1     2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "2     2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "3     2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "4     2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "...          ...           ...           ...      ...       ...     ...   \n",
       "18243 2018-02-18          1.56      17597.12  1892.05   1928.36    0.00   \n",
       "18244 2018-02-25          1.57      18421.24  1974.26   2482.65    0.00   \n",
       "18245 2018-03-04          1.54      17393.30  1832.24   1905.57    0.00   \n",
       "18246 2018-03-11          1.56      22128.42  2162.67   3194.25    8.93   \n",
       "18247 2018-03-18          1.56      15896.38  2055.35   1499.55    0.00   \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0         9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "1         8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "2        11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "3        10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "4         9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "...           ...         ...         ...          ...           ...   ...   \n",
       "18243    13776.71    13553.53      223.18          0.0       organic  2018   \n",
       "18244    13964.33    13698.27      266.06          0.0       organic  2018   \n",
       "18245    13655.49    13401.93      253.56          0.0       organic  2018   \n",
       "18246    16762.57    16510.32      252.25          0.0       organic  2018   \n",
       "18247    12341.48    12114.81      226.67          0.0       organic  2018   \n",
       "\n",
       "                 region  AveragePriceNextWeek  \n",
       "0                Albany                  1.24  \n",
       "1                Albany                  1.17  \n",
       "2                Albany                  1.06  \n",
       "3                Albany                  0.99  \n",
       "4                Albany                  0.99  \n",
       "...                 ...                   ...  \n",
       "18243  WestTexNewMexico                  1.57  \n",
       "18244  WestTexNewMexico                  1.54  \n",
       "18245  WestTexNewMexico                  1.56  \n",
       "18246  WestTexNewMexico                  1.56  \n",
       "18247  WestTexNewMexico                  1.62  \n",
       "\n",
       "[18141 rows x 14 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hastarget = create_lag_feature(df_sort, \"AveragePrice\", +1, [\"region\", \"type\"], \"AveragePriceNextWeek\", clip=True)\n",
    "df_hastarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict `AveragePriceNextWeek`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_hastarget[df_hastarget[\"Date\"] <= split_date]\n",
    "df_test  = df_hastarget[df_hastarget[\"Date\"] >  split_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 `AveragePrice` baseline \n",
    "rubric={points}\n",
    "\n",
    "Soon we will want to build some models to forecast the average avocado price a week in advance. Before we start with any ML though, let's try a baseline. Previously we used `DummyClassifier` or `DummyRegressor` as a baseline. This time, we'll do something else as a baseline: we'll assume the price stays the same from this week to next week. So, we'll set our prediction of \"AveragePriceNextWeek\" exactly equal to \"AveragePrice\", assuming no change. That is kind of like saying, \"If it's raining today then I'm guessing it will be raining tomorrow\". This simplistic approach will not get a great score but it's a good starting point for reference. If our model does worse that this, it must not be very good. \n",
    "\n",
    "Using this baseline approach, what $R^2$ do you get on the train and test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get R^2 = 0.8286 for train data, and 0.7632 for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zd/g1vsl2ds16v0rlg6n5tt1kzw0000gn/T/ipykernel_30130/432108898.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['BaselinePrediction'] = df_train['AveragePrice']\n",
      "/var/folders/zd/g1vsl2ds16v0rlg6n5tt1kzw0000gn/T/ipykernel_30130/432108898.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['BaselinePrediction'] = df_test['AveragePrice']\n"
     ]
    }
   ],
   "source": [
    "df_train['BaselinePrediction'] = df_train['AveragePrice']\n",
    "df_test['BaselinePrediction'] = df_test['AveragePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_r2 = r2_score(df_train['AveragePriceNextWeek'], df_train['BaselinePrediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8285800937261841"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_r2 = r2_score(df_test['AveragePriceNextWeek'], df_test['BaselinePrediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7631780188583048"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert not train_r2 is None, \"Are you using the correct variable name?\"\n",
    "assert not test_r2 is None, \"Are you using the correct variable name?\"\n",
    "assert sha1(str(round(train_r2, 3)).encode('utf8')).hexdigest() == 'b1136fe2a8918904393ab6f40bfb3f38eac5fc39', \"Your training score is not correct. Are you using the right features?\"\n",
    "assert sha1(str(round(test_r2, 3)).encode('utf8')).hexdigest() == 'cc24d9a9b567b491a56b42f7adc582f2eefa5907', \"Your test score is not correct. Are you using the right features?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Forecasting average avocado price\n",
    "rubric={points:10}\n",
    "\n",
    "Now that the baseline is done, let's build some models to forecast the average avocado price a week later. Experiment with a few approachs for encoding the date. Justify the decisions you make. Which approach worked best? Report your test score and briefly discuss your results.\n",
    "\n",
    "Benchmark: you should be able to achieve $R^2$ of at least 0.79 on the test set. I got to 0.80, but not beyond that. Let me know if you do better!\n",
    "\n",
    "Note: because we only have 2 splits here, we need to be a bit wary of overfitting on the test set. Try not to test on it a ridiculous number of times. If you are interested in some proper ways of dealing with this, see for example sklearn's [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), which is like cross-validation for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1 (Basic Date Components): 0.7695\n",
    "Method 2 (Ordinal Encoding): 0.7087\n",
    "Method 3 (Cyclical Encoding): 0.7732\n",
    "Method 4 (Combined Encoding): 0.8350\n",
    "\n",
    "Method 4 (Combined Encoding) worked best with a test R² score of 0.8350. This approach combined AveragePrice, Total Volume, and all types of date encodings (Year, Month, Day, OrdinalDate, Month_sin, Month_cos). The model performs better likely because it captures both linear time trends through basic components and seasonal patterns through cyclical encoding. \n",
    "\n",
    "While all methods show high training scores (around 0.97), Method 4's higher test score indicates better generalization to new data and less overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18141 entries, 0 to 18247\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   Date                  18141 non-null  datetime64[ns]\n",
      " 1   AveragePrice          18141 non-null  float64       \n",
      " 2   Total Volume          18141 non-null  float64       \n",
      " 3   4046                  18141 non-null  float64       \n",
      " 4   4225                  18141 non-null  float64       \n",
      " 5   4770                  18141 non-null  float64       \n",
      " 6   Total Bags            18141 non-null  float64       \n",
      " 7   Small Bags            18141 non-null  float64       \n",
      " 8   Large Bags            18141 non-null  float64       \n",
      " 9   XLarge Bags           18141 non-null  float64       \n",
      " 10  type                  18141 non-null  object        \n",
      " 11  year                  18141 non-null  int64         \n",
      " 12  region                18141 non-null  object        \n",
      " 13  AveragePriceNextWeek  18141 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(10), int64(1), object(2)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_hastarget.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Date Components Results:\n",
      "Train R² Score: 0.9793\n",
      "Test R² Score: 0.7702\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Basic date components\n",
    "# Add date features\n",
    "df_train_dated = df_train.copy()\n",
    "df_test_dated = df_test.copy()\n",
    "\n",
    "df_train_dated['Year'] = df_train_dated['Date'].dt.year\n",
    "df_train_dated['Month'] = df_train_dated['Date'].dt.month\n",
    "df_train_dated['Day'] = df_train_dated['Date'].dt.day\n",
    "\n",
    "df_test_dated['Year'] = df_test_dated['Date'].dt.year\n",
    "df_test_dated['Month'] = df_test_dated['Date'].dt.month\n",
    "df_test_dated['Day'] = df_test_dated['Date'].dt.day\n",
    "\n",
    "base_features = ['AveragePrice', 'Total Volume', '4046', '4225', '4770', \n",
    "                'Total Bags', 'Small Bags', 'Large Bags', 'XLarge Bags', 'type', 'region']\n",
    "\n",
    "features = base_features + ['Year', 'Month', 'Day']\n",
    "X_train = df_train_dated[features]\n",
    "X_test = df_test_dated[features]\n",
    "\n",
    "X_train = pd.get_dummies(X_train, columns=['type', 'region'])\n",
    "X_test = pd.get_dummies(X_test, columns=['type', 'region'])\n",
    "\n",
    "# Align columns\n",
    "common_cols = set(X_train.columns) & set(X_test.columns)\n",
    "X_train = X_train[list(common_cols)]\n",
    "X_test = X_test[list(common_cols)]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, df_train_dated['AveragePriceNextWeek'])\n",
    "\n",
    "train_r2 = r2_score(df_train_dated['AveragePriceNextWeek'], model.predict(X_train_scaled))\n",
    "test_r2 = r2_score(df_test_dated['AveragePriceNextWeek'], model.predict(X_test_scaled))\n",
    "\n",
    "print(\"Basic Date Components Results:\")\n",
    "print(f\"Train R² Score: {train_r2:.4f}\")\n",
    "print(f\"Test R² Score: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal Date Encoding Results:\n",
      "Train R² Score: 0.9793\n",
      "Test R² Score: 0.7098\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Ordinal encoding\n",
    "# Add date features\n",
    "df_train_dated = df_train.copy()\n",
    "df_test_dated = df_test.copy()\n",
    "\n",
    "df_train_dated['OrdinalDate'] = (df_train_dated['Date'] - df_train_dated['Date'].min()).dt.days\n",
    "df_test_dated['OrdinalDate'] = (df_test_dated['Date'] - df_train_dated['Date'].min()).dt.days\n",
    "\n",
    "features = base_features + ['OrdinalDate']\n",
    "X_train = df_train_dated[features]\n",
    "X_test = df_test_dated[features]\n",
    "\n",
    "X_train = pd.get_dummies(X_train, columns=['type', 'region'])\n",
    "X_test = pd.get_dummies(X_test, columns=['type', 'region'])\n",
    "\n",
    "# Align columns\n",
    "common_cols = set(X_train.columns) & set(X_test.columns)\n",
    "X_train = X_train[list(common_cols)]\n",
    "X_test = X_test[list(common_cols)]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, df_train_dated['AveragePriceNextWeek'])\n",
    "\n",
    "train_r2 = r2_score(df_train_dated['AveragePriceNextWeek'], model.predict(X_train_scaled))\n",
    "test_r2 = r2_score(df_test_dated['AveragePriceNextWeek'], model.predict(X_test_scaled))\n",
    "\n",
    "print(\"Ordinal Date Encoding Results:\")\n",
    "print(f\"Train R² Score: {train_r2:.4f}\")\n",
    "print(f\"Test R² Score: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyclical Encoding Results with Lagged Dataset:\n",
      "Train R² Score: 0.9795\n",
      "Test R² Score: 0.7732\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Cyclical encoding with lagged dataset\n",
    "df_train = df_hastarget[df_hastarget[\"Date\"] <= split_date]\n",
    "df_test = df_hastarget[df_hastarget[\"Date\"] > split_date]\n",
    "\n",
    "# Add date features\n",
    "df_train_dated = df_train.copy()\n",
    "df_test_dated = df_test.copy()\n",
    "\n",
    "for df in [df_train_dated, df_test_dated]:\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "    \n",
    "    df['Week'] = df['Date'].dt.isocalendar().week\n",
    "    df['Week_sin'] = np.sin(2 * np.pi * df['Week'] / 52)\n",
    "    df['Week_cos'] = np.cos(2 * np.pi * df['Week'] / 52)\n",
    "\n",
    "features = ['AveragePrice', 'Total Volume', \n",
    "           'Total Bags', 'Small Bags', 'Large Bags', 'XLarge Bags',\n",
    "           'Year', 'Month_sin', 'Month_cos', 'Week_sin', 'Week_cos',\n",
    "           'type', 'region']\n",
    "\n",
    "X_train = df_train_dated[features]\n",
    "X_test = df_test_dated[features]\n",
    "\n",
    "X_train = pd.get_dummies(X_train, columns=['type', 'region'])\n",
    "X_test = pd.get_dummies(X_test, columns=['type', 'region'])\n",
    "\n",
    "common_cols = set(X_train.columns) & set(X_test.columns)\n",
    "X_train = X_train[list(common_cols)]\n",
    "X_test = X_test[list(common_cols)]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, df_train_dated['AveragePriceNextWeek'])\n",
    "\n",
    "train_r2 = r2_score(df_train_dated['AveragePriceNextWeek'], model.predict(X_train_scaled))\n",
    "test_r2 = r2_score(df_test_dated['AveragePriceNextWeek'], model.predict(X_test_scaled))\n",
    "\n",
    "print(\"Cyclical Encoding Results with Lagged Dataset:\")\n",
    "print(f\"Train R² Score: {train_r2:.4f}\")\n",
    "print(f\"Test R² Score: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Encoding Results with Lagged Dataset:\n",
      "Train R^2 Score: 0.9778879870981543\n",
      "Test R^2 Score: 0.8349730055479765\n"
     ]
    }
   ],
   "source": [
    "#Method 4: Combined Encoding\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Add new features\n",
    "df_hastarget['Year'] = df_hastarget['Date'].dt.year\n",
    "df_hastarget['Month'] = df_hastarget['Date'].dt.month\n",
    "df_hastarget['Day'] = df_hastarget['Date'].dt.day\n",
    "df_hastarget['OrdinalDate'] = (df_hastarget['Date'] - df_hastarget['Date'].min()).dt.days\n",
    "\n",
    "# Cyclic encoding\n",
    "df_hastarget['Month_sin'] = np.sin(2 * np.pi * df_hastarget['Month'] / 12)\n",
    "df_hastarget['Month_cos'] = np.cos(2 * np.pi * df_hastarget['Month'] / 12)\n",
    "\n",
    "# Train-test split\n",
    "X = df_hastarget[['AveragePrice', 'Total Volume', 'Year', 'Month', 'Day', 'OrdinalDate', 'Month_sin', 'Month_cos']]\n",
    "y = df_hastarget['AveragePriceNextWeek']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Combined Encoding Results with Lagged Dataset:\")\n",
    "print(f\"Train R^2 Score: {train_r2}\")\n",
    "print(f\"Test R^2 Score: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Short answer questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Time series\n",
    "\n",
    "rubric={points:6}\n",
    "\n",
    "The following questions pertain to Lecture 20 on time series data:\n",
    "\n",
    "1. Sometimes a time series has missing time points or, worse, time points that are unequally spaced in general. Give an example of a real world situation where the time series data would have unequally spaced time points.\n",
    "2. In class we discussed two approaches to using temporal information: encoding the date as one or more features, and creating lagged versions of features. Which of these (one/other/both/neither) two approaches would struggle with unequally spaced time points? Briefly justify your answer.\n",
    "3. When studying time series modeling, we explored several ways to encode date information as a feature for the citibike dataset. When we used time of day as a numeric feature, the Ridge model was not able to capture the periodic pattern. Why? How did we tackle this problem? Briefly explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A real-world example of unequally spaced time points is earthquake data. Earthquakes do not occur at regular intervals; instead, they happen sporadically based on natural geophysical processes. Each recorded event has a specific timestamp, but the time gap between events can vary significantly—from seconds to days, months, or even years.\n",
    "2. Lagged features would struggle more with unequally spaced time points because they assume a consistent interval between observations (e.g., daily, hourly). When time points are unevenly spaced, a lag value may no longer represent a fixed temporal distance (e.g., \"1 lag\" could correspond to 1 day in one case and 3 days in another), leading to inconsistencies that can confuse the model and result in incorrect assumptions about temporal relationships. In contrast, encoding the date as features (e.g., year, month, day, or cyclic encodings like sine/cosine for seasonal patterns) is less affected, as it captures information about the time point itself without relying on fixed temporal gaps, making it more robust to unequal spacing in the data.\n",
    "3. When \"time of day\" was used as a numeric feature (e.g., representing 0 to 23 for hours), the Ridge regression model treated it as a linear numeric variable. However, time of day is a cyclical variable (e.g., 23:00 is closer to 00:00 than 12:00). Linear models like Ridge fail to capture this periodic relationship, resulting in poor performance when modeling patterns like daily bike usage, which is inherently cyclical. To address this, we encoded the time of day cyclically using sine and cosine transformations. Sine and cosine encode the cyclical nature of time (e.g., midnight and 23:59 are mathematically close). These features allow Ridge to capture periodic patterns effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Computer vision \n",
    "rubric={points:6}\n",
    "\n",
    "The following questions pertain to Lecture 19 on multiclass classification and introduction to computer vision. \n",
    "\n",
    "1. How many parameters (coefficients and intercepts) will `sklearn`’s `LogisticRegression()` model learn for a four-class classification problem, assuming that you have 10 features? Briefly explain your answer.\n",
    "2. In Lecture 19, we briefly discussed how neural networks are sort of like `sklearn`'s pipelines, in the sense that they involve multiple sequential transformations of the data, finally resulting in the prediction. Why was this property useful when it came to transfer learning?\n",
    "3. Imagine that you have a small dataset with ~1000 images containing pictures and names of 50 different Computer Science faculty members from UBC. Your goal is to develop a reasonably accurate multi-class classification model for this task. Describe which model/technique you would use and briefly justify your choice in one to three sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 44 parameters because we have 10 coefficients (for 10 features) and 1 intercept for each class, so for a four-class classification, we have 11*4 = 44 parameters.\n",
    "2. Neural networks process data through sequential layers, where early layers learn general features (e.g., edges, textures) and later layers learn task-specific patterns. This structure is useful for transfer learning because we can reuse the general-purpose early layers from a pre-trained model and only retrain the final layers for the new task. This saves computation and reduces the need for large datasets.\n",
    "3. I would use a pre-trained CNN model like ResNet-50 with transfer learning and fine-tuning because: the dataset is too small for training from scratch, and pre-trained CNN models already have good feature extraction capabilities for images. Therefore, we only need to retrain the final layers to adapt it to our 50 faculty member classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before submitting your assignment, please make sure you have followed all the instructions in the Submission instructions section at the top.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330] *",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "name": "_merged",
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "438px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
